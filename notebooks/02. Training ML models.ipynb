{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML models\n",
    "\n",
    "In this notebook we use the `./final/dataset.npz` dataset to train multiple ML models to create a surrogate models. The comparison between models will be presented in the next notebook.\n",
    "\n",
    "During training process dataset is split into train, test and validate sets of sizes `700000`, `150000` and `150000`.\n",
    "\n",
    "List of tested methods (with links to used implementations):\n",
    "1. [Neural Networks](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "2. Linear regression model:\n",
    "    1. [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) L1 and L2 regularization combined\n",
    "3. Decision Trees models:\n",
    "    1. [Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)\n",
    "    2. [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)\n",
    "    3. [Random Forrest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "    4. [Extra Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)\n",
    "4. [k-nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "For each ML method we use [Optuna](https://optuna.readthedocs.io) to find best performing set of parameters.\n",
    "\n",
    "## Running whole notebook may take long time\n",
    "During our our experiments run time was around 48 hours. To test if notebook is working fine, set `RUN_ITERATIONS_PERCENT` to small number (e.g. 3) and run only 3% of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ITERATIONS_PERCENT = 100  # runs only X percent of iterations. pick small number to test if notebook is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (1000000, 11) dtype: float64, outputs shape: (1000000, 200), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "DATA_PATH = Path(\"../final/\").resolve()\n",
    "SQLITE_DB = f\"sqlite:///{DATA_PATH}/optuna.db\"\n",
    "\n",
    "(DATA_PATH / \"ml_models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "input_and_output = np.load(DATA_PATH / \"dataset.npz\")\n",
    "inputs = input_and_output[\"inputs\"].astype(np.float64)\n",
    "outputs = input_and_output[\"outputs\"].astype(np.float64)\n",
    "\n",
    "print(\n",
    "    f\"inputs shape: {inputs.shape} dtype: {inputs.dtype}, outputs shape: {outputs.shape}, dtype: {outputs.dtype}\"\n",
    ")\n",
    "dataset_size = inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test output [1.e-08 1.e-05 1.e-10 1.e-09 1.e+00 2.e+00 1.e+01 0.e+00]\n",
      "transformed output: [-7.      -5.      -7.      -7.       0.       0.30103  1.      -7.     ]\n",
      "original output is untouched after transform: [1.e-08 1.e-05 1.e-10 1.e-09 1.e+00 2.e+00 1.e+01 0.e+00]\n",
      "transformed and untransformed output: [0.e+00 1.e-05 0.e+00 0.e+00 1.e+00 2.e+00 1.e+01 0.e+00]\n"
     ]
    }
   ],
   "source": [
    "# In this problem we are interested in order of magnitude rather than absolute value of the tumour size.\n",
    "# To train the models output is first transformed with log_10. When the tumour size is smaller than 10^-9 L,\n",
    "# there is no way to find it, so we can limit the lower bound of tumour size with 10^-9\n",
    "\n",
    "LOWER_LIMIT = -7\n",
    "\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "\n",
    "\n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs <= LOWER_LIMIT\n",
    "    z = 10**transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "\n",
    "test_output = np.array([10 ** (-8), 10 ** (-5), 10 ** (-10), 10 ** (-9), 1, 2, 10, 0.0])\n",
    "\n",
    "print(f\"test output {test_output}\")\n",
    "print(f\"transformed output: {output_transform(test_output)}\")\n",
    "print(f\"original output is untouched after transform: {test_output}\")\n",
    "print(\n",
    "    f\"transformed and untransformed output: {output_untransform(output_transform(test_output))}\"\n",
    ")\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data has an extra column with all ones - that we get rid of before training the model\n",
    "\n",
    "\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sizes: (700000, 10), (700000, 200)\n",
      "test sizes: (150000, 10), (150000, 200)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train (70%), test (15%), validate (15%) subsets\n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "print(f\"train sizes: {X_train.shape}, {Y_train.shape}\")\n",
    "X_test = input_without_treatment[train_size : (train_size + test_size), :]\n",
    "Y_test = outputs_order_of_magnitude[train_size : (train_size + test_size), :]\n",
    "print(f\"test sizes: {X_test.shape}, {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling inputs\n",
    "from custom_scaler import get_scaler\n",
    "\n",
    "scaler = get_scaler(DATA_PATH, X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.65866883e-01 3.10364783e-02 2.12998980e-03 6.91833518e-04\n",
      " 1.62496763e-04 5.55698571e-05 2.52885361e-05 1.32508249e-05\n",
      " 6.56014178e-06 3.97412630e-06 2.41222953e-06 1.56883509e-06\n",
      " 1.00395773e-06 6.69877073e-07 4.67876722e-07 3.31892112e-07]\n",
      "[0.03413311696416754, 0.003096638699825127, 0.0009666489008957665, 0.00027481538295305835, 0.0001123186198628594, 5.67487627470774e-05, 3.146022667338151e-05, 1.8209401804249853e-05, 1.1649260025332503e-05, 7.675133723264175e-06, 5.262904194529818e-06, 3.6940691008915433e-06, 2.6901113730976307e-06, 2.0202342996103437e-06, 1.5523575778771536e-06, 1.2204654663490287e-06]\n"
     ]
    }
   ],
   "source": [
    "# applying pca to outputs\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "PCA_COMPONENTS = 16\n",
    "\n",
    "pca_path = DATA_PATH / f\"pca{PCA_COMPONENTS}.pickle\"\n",
    "\n",
    "if pca_path.exists():\n",
    "    with pca_path.open(\"rb\") as opened_file:\n",
    "        pca = pickle.load(opened_file)\n",
    "    Y_train_pca = pca.transform(Y_train)\n",
    "else:\n",
    "    pca = PCA(n_components=PCA_COMPONENTS)\n",
    "    Y_train_pca = pca.fit_transform(Y_train)\n",
    "    with pca_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(pca, opened_file)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "from functools import reduce\n",
    "\n",
    "print(\n",
    "    list(reduce(lambda a, b: a + [a[-1] - b], pca.explained_variance_ratio_, [1.0]))[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. no exception\n",
      "2. message: timeout (14, <frame at 0x7f70b67803e0, file '/tmp/ipykernel_377653/1902325977.py', line 35, code <module>>)\n"
     ]
    }
   ],
   "source": [
    "# Example of limiting running time of the loop by scheduling an sigalrm and adding a handler for it.\n",
    "\n",
    "import optuna\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import psutil\n",
    "import os\n",
    "import signal\n",
    "\n",
    "\n",
    "def timeout_handler(*args):\n",
    "    raise TimeoutError(f\"timeout {args}\")\n",
    "\n",
    "\n",
    "def keyboard_interrupt_handler(*args):\n",
    "    os.kill(os.getpid(), signal.SIGINT)\n",
    "\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "try:\n",
    "    signal.alarm(2)\n",
    "    time.sleep(1)\n",
    "    signal.alarm(0)\n",
    "    print(f\"1. no exception\")\n",
    "except TimeoutError as e:\n",
    "    print(f\"1. message: {e}\")\n",
    "except KeyboardInterrupt as e:\n",
    "    print(f\"1. KEYBOARD: {e}\")\n",
    "\n",
    "try:\n",
    "    signal.alarm(2)\n",
    "    time.sleep(5)\n",
    "    signal.alarm(0)\n",
    "    print(f\"2. no exception\")\n",
    "except TimeoutError as e:\n",
    "    print(f\"2. message: {e}\")\n",
    "except KeyboardInterrupt as e:\n",
    "    print(f\"2. keyboard: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model, error, trial):\n",
    "    try:\n",
    "        best_value = trial.study.best_value\n",
    "    except:\n",
    "        best_value = float(\"inf\")\n",
    "    if error < best_value:\n",
    "        print(f\"Updating {trial.study.study_name}.pickle\")\n",
    "        with (DATA_PATH / f\"{trial.study.study_name}.pickle\").open(\"wb\") as file_obj:\n",
    "            pickle.dump(model, file_obj)\n",
    "\n",
    "\n",
    "def mse_on_test_dataset(model):\n",
    "    Y_predict_pca = model.predict(X_test_scaled)\n",
    "    Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "    return mean_squared_error(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 12:01:26,313]\u001b[0m Using an existing study with name 'MLPRegressor' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MLPRegressor test dataset error: 8.987444574725626e-05 best_params: {'alpha': 0.0029879519050999255, 'layer1/3': 800, 'layer2/3': 350, 'layer3/3': 30, 'learning_rate': 'adaptive', 'learning_rate_init': 0.00035901468189128335}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization for neural network\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "\n",
    "common_params = {\n",
    "    \"tol\": 3e-6,\n",
    "    \"n_iter_no_change\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"warm_start\": False,\n",
    "    \"batch_size\": 10000,\n",
    "    \"max_iter\": 5000,\n",
    "}\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    global common_params\n",
    "\n",
    "    phase = min((trial.number // 30), 3)\n",
    "    training_sizes = (0.05, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (300, 600, 1200, 3600)\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "\n",
    "    # learning_rate = trial.suggest_categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"])\n",
    "    learning_rate = \"constant\"\n",
    "    model_params = {\n",
    "        **common_params,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.001, 1.0, log=True),\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"learning_rate_init\": trial.suggest_float(\n",
    "            \"learning_rate_init\", 0.0001, 0.1, log=True\n",
    "        ),\n",
    "        \"power_t\": trial.suggest_float(\"power_t\", 0.1, 2.0, log=True)\n",
    "        if learning_rate == \"invscaling\"\n",
    "        else 0.5,\n",
    "        \"hidden_layer_sizes\": [\n",
    "            trial.suggest_int(f\"layer1/3\", 600, 1200, step=200),\n",
    "            trial.suggest_int(f\"layer2/3\", 200, 600, step=50),\n",
    "            trial.suggest_int(f\"layer3/3\", 10, 30, step=5),\n",
    "        ],\n",
    "        \"random_state\": trial.suggest_categorical(\"random_state\", list(range(0, 32))),\n",
    "        #         \"hidden_layer_sizes\": [700, 200, 30]\n",
    "    }\n",
    "\n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    trial.set_user_attr(\"model_params\", model_params)\n",
    "\n",
    "    model = MLPRegressor(**trial.user_attrs[\"model_params\"])\n",
    "\n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        with threadpool_limits(limits=8, user_api=\"blas\"):\n",
    "            model.fit(X_train_scaled[:training_size, :], Y_train_pca[:training_size, :])\n",
    "        signal.alarm(0)\n",
    "        error = mse_on_test_dataset(model)\n",
    "        save_best_model(model, error, trial)\n",
    "\n",
    "    except TimeoutError:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"MLPRegressor\",\n",
    "    storage=SQLITE_DB,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "trials_due = 110 * RUN_ITERATIONS_PERCENT // 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    previous_handler = signal.signal(signal.SIGALRM, keyboard_interrupt_handler)\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "    signal.signal(signal.SIGALRM, previous_handler)\n",
    "\n",
    "print(\n",
    "    f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 12:01:26,445]\u001b[0m A new study created in RDB with name: MLPRegressor_constant_300\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tol': 1e-05, 'n_iter_no_change': 5, 'random_state': 30, 'warm_start': False, 'max_iter': 2000, 'batch_size': 25000, 'learning_rate': 'constant', 'alpha': 0.006101610233741692, 'learning_rate_init': 0.006839644538652477, 'power_t': 0.5, 'hidden_layer_sizes': [800, 400, 50]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czaki/mambaforge/envs/qsp_emulation/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "\u001b[32m[I 2023-04-07 12:06:29,209]\u001b[0m Trial 0 finished with value: 0.0102477563408473 and parameters: {'alpha': 0.006101610233741692, 'learning_rate_init': 0.006839644538652477, 'layer1/3': 800, 'layer2/3': 400, 'layer3/3': 50, 'batch_size': 25000, 'random_state': 30}. Best is trial 0 with value: 0.0102477563408473.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 12:06:29,239]\u001b[0m Using an existing study with name 'MLPRegressor_invscaling_300' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2023-04-07 12:06:29,283]\u001b[0m Using an existing study with name 'MLPRegressor_adaptive_300' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating MLPRegressor_constant_300.pickle\n",
      "model: MLPRegressor_constant_300 test dataset error: 0.0102477563408473 best_params: {'alpha': 0.006101610233741692, 'batch_size': 25000, 'layer1/3': 800, 'layer2/3': 400, 'layer3/3': 50, 'learning_rate_init': 0.006839644538652477, 'random_state': 30}\n",
      "model: MLPRegressor_invscaling_300 test dataset error: 0.013776365727441831 best_params: {'alpha': 0.04361488288211113, 'batch_size': 20000, 'layer1/3': 1200, 'layer2/3': 550, 'layer3/3': 50, 'learning_rate_init': 0.0005348826277957722, 'power_t': 0.4428082303763544, 'random_state': 25}\n",
      "model: MLPRegressor_adaptive_300 test dataset error: 0.07229927344000217 best_params: {'alpha': 0.002743329373331669, 'batch_size': 25000, 'layer1/3': 1000, 'layer2/3': 500, 'layer3/3': 40, 'learning_rate_init': 0.00014888516824883286, 'random_state': 8}\n"
     ]
    }
   ],
   "source": [
    "# Test how neural network perform with 10 minute training cap on full dataset\n",
    "TIMEOUT_SECONDS = 300\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "for learning_rate in [\"constant\", \"invscaling\", \"adaptive\"]:\n",
    "    common_params = {\n",
    "        \"tol\": 1e-5,\n",
    "        \"n_iter_no_change\": 5,\n",
    "        \"random_state\": 42,\n",
    "        \"warm_start\": False,\n",
    "        \"max_iter\": 2000,\n",
    "        \"batch_size\": 10000,\n",
    "        \"learning_rate\": learning_rate,\n",
    "    }\n",
    "\n",
    "    def objective(trial):\n",
    "        global common_params\n",
    "\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.0001, 1.0, log=True)\n",
    "        learning_rate_init = trial.suggest_float(\n",
    "            \"learning_rate_init\", 0.0001, 0.05, log=True\n",
    "        )\n",
    "        power_t = (\n",
    "            trial.suggest_float(\"power_t\", 0.1, 2.0, log=True)\n",
    "            if learning_rate == \"invscaling\"\n",
    "            else 0.5\n",
    "        )\n",
    "\n",
    "        model_params = {\n",
    "            **common_params,\n",
    "            \"alpha\": alpha,\n",
    "            \"learning_rate_init\": learning_rate_init,\n",
    "            \"power_t\": power_t,\n",
    "            \"hidden_layer_sizes\": [\n",
    "                trial.suggest_int(f\"layer1/3\", 600, 1200, step=200),\n",
    "                trial.suggest_int(f\"layer2/3\", 200, 600, step=50),\n",
    "                trial.suggest_int(f\"layer3/3\", 10, 50, step=5),\n",
    "            ],\n",
    "            \"batch_size\": trial.suggest_int(\"batch_size\", 10000, 25000, step=5000),\n",
    "            \"random_state\": trial.suggest_categorical(\n",
    "                \"random_state\", list(range(0, 32))\n",
    "            ),\n",
    "            # \"hidden_layer_sizes\": [600, 200, 50]\n",
    "        }\n",
    "        print(model_params)\n",
    "\n",
    "        trial.set_user_attr(\"model_params\", model_params)\n",
    "\n",
    "        model = MLPRegressor(**trial.user_attrs[\"model_params\"])\n",
    "        trial.set_user_attr(\"training_size\", 1.0)\n",
    "        trial.set_user_attr(\"max_duration_s\", TIMEOUT_SECONDS)\n",
    "\n",
    "        try:\n",
    "            signal.alarm(TIMEOUT_SECONDS)\n",
    "            with threadpool_limits(limits=8, user_api=\"blas\"):\n",
    "                model.fit(X_train_scaled, Y_train_pca)\n",
    "            signal.alarm(0)\n",
    "            error = mse_on_test_dataset(model)\n",
    "            save_best_model(model, error, trial)\n",
    "\n",
    "        except TimeoutError:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        return error\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"MLPRegressor_{learning_rate}_{TIMEOUT_SECONDS}\",\n",
    "        storage=SQLITE_DB,\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    trials_due = 40 * RUN_ITERATIONS_PERCENT // 100 - len(study.trials)\n",
    "    if trials_due > 0:\n",
    "        previous_handler = signal.signal(signal.SIGALRM, keyboard_interrupt_handler)\n",
    "        study.optimize(objective, n_trials=trials_due)\n",
    "        signal.signal(signal.SIGALRM, previous_handler)\n",
    "\n",
    "    print(\n",
    "        f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.delete_study(study_name=\"MLPRegressor_constant_300\", storage=SQLITE_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 12:06:29,388]\u001b[0m Using an existing study with name 'ElasticNet' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ElasticNet test dataset error: 0.06892972685134742 best_params: {'alpha': 5.98103619625795e-05, 'fit_intercept': True, 'l1_ratio': 0.5905390879114973, 'polynomial degree': 3}\n"
     ]
    }
   ],
   "source": [
    "# Linear regression with combined L1 and L2 priors as regularizer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "common_params = {\"random_state\": 42, \"tol\": 1e-5}\n",
    "\n",
    "MAX_POLYNOMIAL_DEGREE = 3\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    global common_params\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (240, 360, 600, 1200)\n",
    "\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "    max_iter = [20000, 40000, 80000, 160000][phase]\n",
    "\n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    trial.set_user_attr(\n",
    "        \"model_params\",\n",
    "        {\n",
    "            **common_params,\n",
    "            \"max_iter\": max_iter,\n",
    "            \"l1_ratio\": trial.suggest_float(\"l1_ratio\", 0.0, 1.0),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 0.00001, 1.0, log=True),\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\n",
    "        \"polynomial degree\",\n",
    "        trial.suggest_int(\"polynomial degree\", 1, MAX_POLYNOMIAL_DEGREE),\n",
    "    )\n",
    "\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(trial.user_attrs[\"polynomial degree\"]),\n",
    "        ElasticNet(**trial.user_attrs[\"model_params\"]),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size, :], Y_train_pca[:training_size, :])\n",
    "        signal.alarm(0)\n",
    "        error = mse_on_test_dataset(model)\n",
    "        save_best_model(model, error, trial)\n",
    "    except TimeoutError:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"ElasticNet\", storage=SQLITE_DB, load_if_exists=True\n",
    ")\n",
    "trials_due = 100 * RUN_ITERATIONS_PERCENT // 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "\n",
    "print(\n",
    "    f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 12:06:29,480]\u001b[0m Using an existing study with name 'HistGradientBoostingRegressor' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: HistGradientBoostingRegressor test dataset error: 0.008577411300669628 best_params: {'l2_regularization': 0.001871946506788565, 'learning_rate': 0.1651977983535292, 'loss': 'squared_error', 'max_iter': 160, 'regularize': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    trial_params = {\n",
    "        \"loss\": trial.suggest_categorical(\n",
    "            # poisson requires y > 0 which is not true in this case\n",
    "            \"loss\",\n",
    "            [\"squared_error\", \"absolute_error\", \"quantile\"],\n",
    "        ),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.00001, 0.5, log=True),\n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 20, 200, step=20),\n",
    "    }\n",
    "\n",
    "    if trial_params[\"loss\"] == \"quantile\":\n",
    "        trial_params[\"quantile\"] = trial.suggest_float(\"quantile\", 0, 1)\n",
    "\n",
    "    if trial.suggest_categorical(\"regularize\", [True, False]):\n",
    "        trial_params[\"l2_regularization\"] = trial.suggest_float(\n",
    "            \"l2_regularization\", 0.00001, 1.0, log=True\n",
    "        )\n",
    "\n",
    "    trial.set_user_attr(\"model_params\", {\"random_state\": 42, **trial_params})\n",
    "\n",
    "    model = MultiOutputRegressor(\n",
    "        HistGradientBoostingRegressor(**trial.user_attrs[\"model_params\"])\n",
    "    )\n",
    "\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (240, 360, 600, 1200)\n",
    "\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "\n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "\n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size, :], Y_train_pca[:training_size, :])\n",
    "        signal.alarm(0)\n",
    "        error = mse_on_test_dataset(model)\n",
    "        save_best_model(model, error, trial)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"HistGradientBoostingRegressor\",\n",
    "    storage=SQLITE_DB,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "trials_due = 100 * RUN_ITERATIONS_PERCENT // 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "\n",
    "print(\n",
    "    f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 12:06:29,631]\u001b[0m Using an existing study with name 'BaggingRegressor' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: BaggingRegressor test dataset error: 0.012123721671364409 best_params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 10, 'max_iter': 50, 'max_samples': 320000, 'oob_score': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.1, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (240, 360, 600, 1200)\n",
    "\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "\n",
    "    trial_params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"max_iter\", 10, 110, step=20),\n",
    "        \"max_samples\": trial.suggest_int(\n",
    "            \"max_samples\", 10000, (training_size // 10000) * 10000, step=10000\n",
    "        ),\n",
    "        \"max_features\": trial.suggest_int(\"max_features\", 1, X_train_scaled.shape[1]),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        \"bootstrap_features\": trial.suggest_categorical(\n",
    "            \"bootstrap_features\", [True, False]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if trial_params[\"bootstrap\"]:\n",
    "        trial_params[\"oob_score\"] = trial.suggest_categorical(\n",
    "            \"oob_score\", [True, False]\n",
    "        )\n",
    "\n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    trial.set_user_attr(\n",
    "        \"model_params\", {\"n_jobs\": -1, \"random_state\": 42, **trial_params}\n",
    "    )\n",
    "\n",
    "    model = MultiOutputRegressor(BaggingRegressor(**trial.user_attrs[\"model_params\"]))\n",
    "\n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size, :], Y_train_pca[:training_size, :])\n",
    "        signal.alarm(0)\n",
    "        error = mse_on_test_dataset(model)\n",
    "        save_best_model(model, error, trial)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "\n",
    "# try:\n",
    "#     optuna.delete_study(study_name=\"BaggingRegressor\", storage=SQLITE_DB)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"BaggingRegressor\",\n",
    "    storage=SQLITE_DB,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "trials_due = 100 * RUN_ITERATIONS_PERCENT // 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "\n",
    "print(\n",
    "    f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 12:06:29,710]\u001b[0m Using an existing study with name 'RandomForrest' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: RandomForrest test dataset error: 0.0276166738609489 best_params: {'bootstrap': True, 'criterion': 'friedman_mse', 'max_features': 4, 'max_samples': 0.1753336890830068, 'min_samples_split': 2, 'n_estimators': 60}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.01, 0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (120, 180, 360, 600, 1200)\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "\n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "    trial.set_user_attr(\n",
    "        \"model_params\",\n",
    "        {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200, step=10),\n",
    "            \"criterion\": trial.suggest_categorical(\n",
    "                \"criterion\", [\"squared_error\", \"absolute_error\", \"friedman_mse\"]\n",
    "            ),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 5),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True]),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.0, 0.2),\n",
    "            \"max_features\": trial.suggest_int(\n",
    "                \"max_features\", 1, X_train_scaled.shape[1] // 2\n",
    "            ),\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": 42,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    model = RandomForestRegressor(**trial.user_attrs[\"model_params\"])\n",
    "\n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size, :], Y_train_pca[:training_size, :])\n",
    "        Y_predict_pca = model.predict(X_test_scaled)\n",
    "        Y_predict = pca.inverse_transform(Y_predict_pca)\n",
    "        error = mean_squared_error(Y_test, Y_predict)\n",
    "        signal.alarm(0)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"RandomForrest\",\n",
    "    storage=SQLITE_DB,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "trials_due = 100 * RUN_ITERATIONS_PERCENT // 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "\n",
    "print(\n",
    "    f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 12:06:29,797]\u001b[0m Using an existing study with name 'ExtraTreesRegressor' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ExtraTreesRegressor test dataset error: 0.035638099610715714 best_params: {'bootstrap': True, 'criterion': 'friedman_mse', 'max_features': 5, 'max_samples': 0.18501919599037991, 'min_samples_split': 2, 'n_estimators': 182}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.01, 0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (120, 180, 360, 600, 1200)\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "    trial.set_user_attr(\"training_size\", training_size)\n",
    "    trial.set_user_attr(\"max_duration_s\", max_duration_s)\n",
    "\n",
    "    trial.set_user_attr(\n",
    "        \"model_params\",\n",
    "        {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
    "            \"criterion\": trial.suggest_categorical(\n",
    "                \"criterion\", [\"squared_error\", \"absolute_error\", \"friedman_mse\"]\n",
    "            ),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 5),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True]),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.0, 0.2),\n",
    "            \"max_features\": trial.suggest_int(\n",
    "                \"max_features\", 1, X_train_scaled.shape[1] // 2\n",
    "            ),\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": 42,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    model = ExtraTreesRegressor(**trial.user_attrs[\"model_params\"])\n",
    "\n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size, :], Y_train_pca[:training_size, :])\n",
    "        signal.alarm(0)\n",
    "        error = mse_on_test_dataset(model)\n",
    "        save_best_model(model, error, trial)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"ExtraTreesRegressor\",\n",
    "    storage=SQLITE_DB,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "trials_due = 100 * RUN_ITERATIONS_PERCENT // 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "\n",
    "print(\n",
    "    f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 12:06:29,878]\u001b[0m Using an existing study with name 'KNeighborsRegressor' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: KNeighborsRegressor test dataset error: 0.21196498486521023 best_params: {'algorithm': 'kd_tree', 'leaf_size': 13, 'n_neighbors': 9, 'p': 3, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "common_params = {\"n_jobs\": -1}\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    phase = min((trial.number // 30), 4)\n",
    "\n",
    "    training_sizes = (0.01, 0.03, 0.1, 0.3, 1.0)\n",
    "    training_max_duration_s = (120, 180, 360, 600, 1200)\n",
    "    training_size = int(training_sizes[phase] * X_train_scaled.shape[0])\n",
    "    max_duration_s = training_max_duration_s[phase]\n",
    "\n",
    "    trial_params = {}\n",
    "    trial_params[\"n_neighbors\"] = trial.suggest_int(\"n_neighbors\", 1, 100)\n",
    "    trial_params[\"weights\"] = trial.suggest_categorical(\n",
    "        \"weights\", [\"uniform\", \"distance\"]\n",
    "    )\n",
    "    trial_params[\"algorithm\"] = trial.suggest_categorical(\n",
    "        \"algorithm\", [\"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "    )\n",
    "    if trial_params[\"algorithm\"] != \"brute\":\n",
    "        trial_params[\"leaf_size\"] = trial.suggest_int(\"leaf_size\", 10, 50)\n",
    "    trial_params[\"p\"] = trial.suggest_int(\"p\", 1, 5)\n",
    "\n",
    "    model_params = {**trial_params, **common_params}\n",
    "    trial.set_user_attr(\"model_params\", model_params)\n",
    "    model = KNeighborsRegressor(**trial.user_attrs[\"model_params\"])\n",
    "\n",
    "    try:\n",
    "        signal.alarm(max_duration_s)\n",
    "        model.fit(X_train_scaled[:training_size, :], Y_train_pca[:training_size, :])\n",
    "        error = mse_on_test_dataset(model)\n",
    "        signal.alarm(0)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"KNeighborsRegressor\",\n",
    "    storage=SQLITE_DB,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "trials_due = 100 * RUN_ITERATIONS_PERCENT // 100 - len(study.trials)\n",
    "if trials_due > 0:\n",
    "    study.optimize(objective, n_trials=trials_due)\n",
    "\n",
    "print(\n",
    "    f\"model: {study.study_name} test dataset error: {study.best_value} best_params: {study.best_params}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
