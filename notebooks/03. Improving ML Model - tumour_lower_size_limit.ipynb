{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657032a4",
   "metadata": {},
   "source": [
    "In this notebook we have hand-adjusted the params of best performing ML model (Neural network implemenation `MLPRegressor`) selected from multiple models in notebook `02. Training ML models` to get even better performing one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9002c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset\n",
      "transformed to orders of magnitude\n",
      "dropped treatment column\n",
      "train sizes: (700000, 10), (700000, 200)\n",
      "test sizes: (150000, 10), (150000, 200)\n",
      "train test split\n",
      "scaled\n",
      "applied pca with 12 components. Unexplained variance ratio: 2.5318616929978012e-06\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 5\n",
    "LOWER_LIMIT = -8\n",
    "PCA_COMPONENTS=12\n",
    "\n",
    "#loading dataset\n",
    "import numpy as np\n",
    "\n",
    "input_and_output = np.load(\"../final/dataset.npz\")\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "outputs = input_and_output[\"outputs\"].astype(np.float64)\n",
    "dataset_size = inputs.shape[0]\n",
    "\n",
    "print(\"loaded dataset\")\n",
    "\n",
    "# transforming time profiles to its orders of magnitude\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "    \n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs <= LOWER_LIMIT\n",
    "    z = 10 ** transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "def apply_size_limit(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    x[x < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return x\n",
    "\n",
    "def apply_absolute_size_limit(outputs: np.array) -> np.array:\n",
    "    limit = 10 ** LOWER_LIMIT\n",
    "    x = np.copy(outputs)\n",
    "    x[x < limit] = 0\n",
    "    return x\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)\n",
    "print(\"transformed to orders of magnitude\")\n",
    "\n",
    "# dropping treatment column in input\n",
    "\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)\n",
    "\n",
    "print(\"dropped treatment column\")\n",
    "\n",
    "#splitting data into train, test, validate datasets \n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "Y_train_absolute = apply_absolute_size_limit(outputs[:train_size, :])\n",
    "print(f\"train sizes: {X_train.shape}, {Y_train.shape}\")\n",
    "X_test = input_without_treatment[train_size:(train_size + test_size), :]\n",
    "Y_test = outputs_order_of_magnitude[train_size:(train_size + test_size), :]\n",
    "Y_test_absolute = apply_absolute_size_limit(outputs[train_size:(train_size + test_size), :])\n",
    "print(f\"test sizes: {X_test.shape}, {Y_test.shape}\")\n",
    "\n",
    "print(\"train test split\")\n",
    "\n",
    "# scaling inputs\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "LOGNORMAL_PARAMETERS = (1, 2)\n",
    "\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.plot_loval = [0.0] * len(LOGNORMAL_PARAMETERS)\n",
    "        self.plot_hival = [1.0] * len(LOGNORMAL_PARAMETERS)\n",
    "\n",
    "    def transform(self, x: np.ndarray, copy=None) -> np.ndarray:\n",
    "        res = self.scaler.transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = (x[:, parameter_index] - self.plot_loval[i]) / (self.plot_hival[i] - self.plot_loval[i])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, x, copy=None):\n",
    "        self.scaler.fit(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            column_values = x[:, parameter_index]\n",
    "\n",
    "            quantile_1, quantile_3 = np.quantile(column_values, [0.25, 0.75], axis=0)\n",
    "            iqr = quantile_3 - quantile_1\n",
    "\n",
    "            loval = quantile_1 - 1.5 * iqr\n",
    "            hival = quantile_3 + 1.5 * iqr\n",
    "\n",
    "            wiskhi = np.compress(column_values <= hival, column_values)\n",
    "            wisklo = np.compress(column_values >= loval, column_values)\n",
    "            actual_hival = np.max(wiskhi)\n",
    "            actual_loval = np.min(wisklo)\n",
    "\n",
    "            self.plot_loval[i] = actual_loval\n",
    "            self.plot_hival[i] = actual_hival\n",
    "\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, x, copy=None):\n",
    "        res = self.scaler.inverse_transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = x[:, parameter_index] * (self.plot_hival[i] - self.plot_loval[i]) + self.plot_loval[i]\n",
    "        return res\n",
    "\n",
    "scaler_path = Path(f\"../final/scaler.pickle\")\n",
    "scaler = None\n",
    "if scaler_path.exists():\n",
    "    with scaler_path.open(\"rb\") as scaler_file:\n",
    "        scaler = pickle.load(scaler_file)\n",
    "else:\n",
    "    scaler = CustomScaler().fit(X_train)\n",
    "    with scaler_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(scaler, opened_file)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"scaled\")\n",
    "\n",
    "# applying principal component analysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_path = Path(f\"../final/pca{PCA_COMPONENTS}_{LOWER_LIMIT}.pickle\")\n",
    "\n",
    "if pca_path.exists():\n",
    "    with pca_path.open(\"rb\") as opened_file:\n",
    "        pca = pickle.load(opened_file)\n",
    "    Y_train_pca = pca.transform(Y_train)\n",
    "else: \n",
    "    pca = PCA(n_components=PCA_COMPONENTS)\n",
    "    Y_train_pca = pca.fit_transform(Y_train)\n",
    "    with pca_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(pca, opened_file)\n",
    "\n",
    "from functools import reduce\n",
    "print(f\"applied pca with {PCA_COMPONENTS} components. Unexplained variance ratio: {reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0)}\")\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a14374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading previous ../final/MLPRegressor_600_100_40_12_-8_0.pickle\n",
      "error test: 7.95509833552666e-05, error train: 7.578464587173801e-05 training_time: 99.7\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-8_0.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.0040005316095293,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 1e-08,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 0.00016798744315656234,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 1e-05,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": false\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 2.5318616929978012e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.5313026484748556e-08,\n",
      "    \"test_error_orders_of_magnitude\": 7.95509833552666e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 2.5590254242361035e-08,\n",
      "    \"train_error_orders_of_magnitude\": 7.578464587173801e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-08,\n",
      "    \"tumour_lower_size_limit_log10_l\": -8\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-8_1.pickle\n",
      "error test: 4.976070291466626e-05, error train: 4.624160875930905e-05 training_time: 205.6\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-8_1.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.00200026580476465,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 5e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 8.399372157828117e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 5e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 2.5318616929978012e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.0781128156107221e-08,\n",
      "    \"test_error_orders_of_magnitude\": 4.976070291466626e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 2.0542437229179557e-08,\n",
      "    \"train_error_orders_of_magnitude\": 4.624160875930905e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-08,\n",
      "    \"tumour_lower_size_limit_log10_l\": -8\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-8_2.pickle\n",
      "error test: 4.756687187137365e-05, error train: 4.4000776626655054e-05 training_time: 312.5\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-8_2.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.001000132902382325,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 2.5e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 4.1996860789140585e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 2.5e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 2.5318616929978012e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.0799132976468798e-08,\n",
      "    \"test_error_orders_of_magnitude\": 4.756687187137365e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 2.02579368766213e-08,\n",
      "    \"train_error_orders_of_magnitude\": 4.4000776626655054e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-08,\n",
      "    \"tumour_lower_size_limit_log10_l\": -8\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-8_3.pickle\n",
      "error test: 4.560937754967478e-05, error train: 4.2106657393594544e-05 training_time: 418.1\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-8_3.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.0005000664511911625,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 1.25e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 2.0998430394570292e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 1.25e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 2.5318616929978012e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.0295545912123433e-08,\n",
      "    \"test_error_orders_of_magnitude\": 4.560937754967478e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 1.9144630181436907e-08,\n",
      "    \"train_error_orders_of_magnitude\": 4.2106657393594544e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-08,\n",
      "    \"tumour_lower_size_limit_log10_l\": -8\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-8_4.pickle\n",
      "error test: 4.4008002234077516e-05, error train: 4.0532013736657303e-05 training_time: 524.2\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-8_4.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.00025003322559558126,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 6.25e-10,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 1.0499215197285146e-05,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 6.25e-07,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 2.5318616929978012e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.0093222568928957e-08,\n",
      "    \"test_error_orders_of_magnitude\": 4.4008002234077516e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 1.835573887695596e-08,\n",
      "    \"train_error_orders_of_magnitude\": 4.0532013736657303e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-08,\n",
      "    \"tumour_lower_size_limit_log10_l\": -8\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-8_5.pickle\n",
      "error test: 4.374403706923748e-05, error train: 4.0218135966212295e-05 training_time: 630.4\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-8_5.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.00012501661279779063,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 3.125e-10,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 5.249607598642573e-06,\n",
      "        \"max_iter\": 300,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 3.125e-07,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 2.5318616929978012e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 9.99092390757325e-09,\n",
      "    \"test_error_orders_of_magnitude\": 4.374403706923748e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 1.8320230359791576e-08,\n",
      "    \"train_error_orders_of_magnitude\": 4.0218135966212295e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-08,\n",
      "    \"tumour_lower_size_limit_log10_l\": -8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from threadpoolctl import threadpool_limits\n",
    "from cpuinfo import get_cpu_info\n",
    "import json\n",
    "\n",
    "\n",
    "hidden_layer_sizes = [600, 100, 40]\n",
    "training_start = time.time()\n",
    "for k in range(6):\n",
    "    last_file = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.pickle\"\n",
    "    info_filename = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.json\"\n",
    "    \n",
    "    if Path(last_file).exists():\n",
    "        print(f\"loading previous {last_file}\")\n",
    "        with Path(last_file).open(\"rb\") as opened_file:\n",
    "            model = pickle.load(opened_file)\n",
    "        with Path(info_filename).open('r') as opened_file:\n",
    "            print(opened_file.read())\n",
    "        continue\n",
    "    \n",
    "    if k > 0:\n",
    "        old_model = model\n",
    "    model_params = {\n",
    "        \"alpha\": 0.0040005316095293 / (2 ** k),\n",
    "        \"batch_size\": 2000,\n",
    "        \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "        \"learning_rate\": \"constant\",\n",
    "        \"learning_rate_init\": 0.00016798744315656234 / (2 ** k),\n",
    "        \"max_iter\": 300,\n",
    "        \"n_iter_no_change\": 5,\n",
    "        \"random_state\": 42,\n",
    "        \"tol\": 1e-05 / (2**k),\n",
    "        \"epsilon\": 1e-08 / (2**k),\n",
    "        \"verbose\": True,\n",
    "        \"warm_start\": k > 0\n",
    "    }\n",
    "    model = MLPRegressor(**model_params)\n",
    "    if k > 0:\n",
    "        for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "            setattr(model, variable_name, getattr(old_model, variable_name))\n",
    "        \n",
    "    with threadpool_limits(limits=get_cpu_info()[\"count\"], user_api='blas'):\n",
    "        model.fit(X_train_scaled, Y_train_pca)\n",
    "        error_test  = mean_squared_error(Y_test,  apply_size_limit(pca.inverse_transform(model.predict(X_test_scaled))))\n",
    "        error_train = mean_squared_error(Y_train, apply_size_limit(pca.inverse_transform(model.predict(X_train_scaled))))\n",
    "        error_test_absolute  = mean_squared_error(Y_test_absolute,  output_untransform(pca.inverse_transform(model.predict(X_test_scaled))))\n",
    "        error_train_absolute = mean_squared_error(Y_train_absolute, output_untransform(pca.inverse_transform(model.predict(X_train_scaled))))\n",
    "\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start:.1f}\")\n",
    "        \n",
    "    with Path(last_file).open(\"wb\") as opened_file:\n",
    "        print(f\"saving {last_file}\")\n",
    "        pickle.dump(model, opened_file)\n",
    "    with Path(info_filename).open('w') as opened_file:\n",
    "        info = json.dumps({\n",
    "            \"cpu_info\": {key: get_cpu_info()[key] for key in [\"arch\", \"bits\", \"brand_raw\", \"count\", \"l2_cache_size\"]},\n",
    "            \"pca_components\": PCA_COMPONENTS,\n",
    "            \"pca_unexplained_variance_ratio\": reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0),\n",
    "            \"tumour_lower_size_limit_l\": 10 ** LOWER_LIMIT,\n",
    "            \"tumour_lower_size_limit_log10_l\": LOWER_LIMIT,\n",
    "            \"model_params\": model_params,\n",
    "            \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
    "            \"test_error_orders_of_magnitude\": error_test,\n",
    "            \"test_error_absolute\": error_test_absolute,\n",
    "            \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
    "            \"train_error_orders_of_magnitude\": error_train,\n",
    "            \"train_error_absolute\": error_train_absolute\n",
    "        }, sort_keys=True, indent=4)\n",
    "        print(f\"saving info to file: {info_filename} {info}\")\n",
    "        opened_file.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60bb70d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed to orders of magnitude\n",
      "dropped treatment column\n",
      "train sizes: (700000, 10), (700000, 200)\n",
      "test sizes: (150000, 10), (150000, 200)\n",
      "train test split\n",
      "scaled\n",
      "applied pca with 12 components. Unexplained variance ratio: 3.734028192096107e-06\n"
     ]
    }
   ],
   "source": [
    "LOWER_LIMIT = -7\n",
    "PCA_COMPONENTS=12\n",
    "\n",
    "# transforming time profiles to its orders of magnitude\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "    \n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs <= LOWER_LIMIT\n",
    "    z = 10 ** transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "def apply_size_limit(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    x[x < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return x\n",
    "\n",
    "def apply_absolute_size_limit(outputs: np.array) -> np.array:\n",
    "    limit = 10 ** LOWER_LIMIT\n",
    "    x = np.copy(outputs)\n",
    "    x[x < limit] = 0\n",
    "    return x\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)\n",
    "print(\"transformed to orders of magnitude\")\n",
    "\n",
    "# dropping treatment column in input\n",
    "\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)\n",
    "\n",
    "print(\"dropped treatment column\")\n",
    "\n",
    "#splitting data into train, test, validate datasets \n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "Y_train_absolute = apply_absolute_size_limit(outputs[:train_size, :])\n",
    "print(f\"train sizes: {X_train.shape}, {Y_train.shape}\")\n",
    "X_test = input_without_treatment[train_size:(train_size + test_size), :]\n",
    "Y_test = outputs_order_of_magnitude[train_size:(train_size + test_size), :]\n",
    "Y_test_absolute = apply_absolute_size_limit(outputs[train_size:(train_size + test_size), :])\n",
    "print(f\"test sizes: {X_test.shape}, {Y_test.shape}\")\n",
    "\n",
    "print(\"train test split\")\n",
    "\n",
    "# scaling inputs\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "LOGNORMAL_PARAMETERS = (1, 2)\n",
    "\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.plot_loval = [0.0] * len(LOGNORMAL_PARAMETERS)\n",
    "        self.plot_hival = [1.0] * len(LOGNORMAL_PARAMETERS)\n",
    "\n",
    "    def transform(self, x: np.ndarray, copy=None) -> np.ndarray:\n",
    "        res = self.scaler.transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = (x[:, parameter_index] - self.plot_loval[i]) / (self.plot_hival[i] - self.plot_loval[i])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, x, copy=None):\n",
    "        self.scaler.fit(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            column_values = x[:, parameter_index]\n",
    "\n",
    "            quantile_1, quantile_3 = np.quantile(column_values, [0.25, 0.75], axis=0)\n",
    "            iqr = quantile_3 - quantile_1\n",
    "\n",
    "            loval = quantile_1 - 1.5 * iqr\n",
    "            hival = quantile_3 + 1.5 * iqr\n",
    "\n",
    "            wiskhi = np.compress(column_values <= hival, column_values)\n",
    "            wisklo = np.compress(column_values >= loval, column_values)\n",
    "            actual_hival = np.max(wiskhi)\n",
    "            actual_loval = np.min(wisklo)\n",
    "\n",
    "            self.plot_loval[i] = actual_loval\n",
    "            self.plot_hival[i] = actual_hival\n",
    "\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, x, copy=None):\n",
    "        res = self.scaler.inverse_transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = x[:, parameter_index] * (self.plot_hival[i] - self.plot_loval[i]) + self.plot_loval[i]\n",
    "        return res\n",
    "\n",
    "scaler_path = Path(f\"../final/scaler.pickle\")\n",
    "scaler = None\n",
    "if scaler_path.exists():\n",
    "    with scaler_path.open(\"rb\") as scaler_file:\n",
    "        scaler = pickle.load(scaler_file)\n",
    "else:\n",
    "    scaler = CustomScaler().fit(X_train)\n",
    "    with scaler_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(scaler, opened_file)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"scaled\")\n",
    "\n",
    "# applying principal component analysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_path = Path(f\"../final/pca{PCA_COMPONENTS}_{LOWER_LIMIT}.pickle\")\n",
    "\n",
    "if pca_path.exists():\n",
    "    with pca_path.open(\"rb\") as opened_file:\n",
    "        pca = pickle.load(opened_file)\n",
    "    Y_train_pca = pca.transform(Y_train)\n",
    "else: \n",
    "    pca = PCA(n_components=PCA_COMPONENTS)\n",
    "    Y_train_pca = pca.fit_transform(Y_train)\n",
    "    with pca_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(pca, opened_file)\n",
    "\n",
    "from functools import reduce\n",
    "print(f\"applied pca with {PCA_COMPONENTS} components. Unexplained variance ratio: {reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0)}\")\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae618b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading previous ../final/MLPRegressor_600_100_40_12_-7_0.pickle\n",
      "error test: 6.564965133072742e-05, error train: 6.08235487423917e-05 training_time: 112.4\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-7_0.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.0040005316095293,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 1e-08,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 0.00016798744315656234,\n",
      "        \"max_iter\": 400,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 1e-05,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": false\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 3.734028192096107e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.2594951489512178e-08,\n",
      "    \"test_error_orders_of_magnitude\": 6.564965133072742e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 2.084464684156289e-08,\n",
      "    \"train_error_orders_of_magnitude\": 6.08235487423917e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-07,\n",
      "    \"tumour_lower_size_limit_log10_l\": -7\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-7_1.pickle\n",
      "error test: 6.182032128073571e-05, error train: 5.730799085325084e-05 training_time: 225.7\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-7_1.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.00200026580476465,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 5e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 8.399372157828117e-05,\n",
      "        \"max_iter\": 400,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 5e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 3.734028192096107e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.128093858236981e-08,\n",
      "    \"test_error_orders_of_magnitude\": 6.182032128073571e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 1.9845098849906343e-08,\n",
      "    \"train_error_orders_of_magnitude\": 5.730799085325084e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-07,\n",
      "    \"tumour_lower_size_limit_log10_l\": -7\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-7_2.pickle\n",
      "error test: 6.39486364816562e-05, error train: 5.94893830214083e-05 training_time: 351.8\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-7_2.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.001000132902382325,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 2.5e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 4.1996860789140585e-05,\n",
      "        \"max_iter\": 400,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 2.5e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 3.734028192096107e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.1509219052200287e-08,\n",
      "    \"test_error_orders_of_magnitude\": 6.39486364816562e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 1.9796413951302564e-08,\n",
      "    \"train_error_orders_of_magnitude\": 5.94893830214083e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-07,\n",
      "    \"tumour_lower_size_limit_log10_l\": -7\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-7_3.pickle\n",
      "error test: 5.585245307461e-05, error train: 5.147691461318252e-05 training_time: 475.0\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-7_3.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.0005000664511911625,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 1.25e-09,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 2.0998430394570292e-05,\n",
      "        \"max_iter\": 400,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 1.25e-06,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 3.734028192096107e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 1.062069063933004e-08,\n",
      "    \"test_error_orders_of_magnitude\": 5.585245307461e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 1.7710336381759846e-08,\n",
      "    \"train_error_orders_of_magnitude\": 5.147691461318252e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-07,\n",
      "    \"tumour_lower_size_limit_log10_l\": -7\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-7_4.pickle\n",
      "error test: 5.08831237005242e-05, error train: 4.662906261783197e-05 training_time: 591.6\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-7_4.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.00025003322559558126,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 6.25e-10,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 1.0499215197285146e-05,\n",
      "        \"max_iter\": 400,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 6.25e-07,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 3.734028192096107e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 9.493366365736659e-09,\n",
      "    \"test_error_orders_of_magnitude\": 5.08831237005242e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 1.5351277365833072e-08,\n",
      "    \"train_error_orders_of_magnitude\": 4.662906261783197e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-07,\n",
      "    \"tumour_lower_size_limit_log10_l\": -7\n",
      "}\n",
      "loading previous ../final/MLPRegressor_600_100_40_12_-7_5.pickle\n",
      "error test: 4.7867532211924434e-05, error train: 4.3621245958808247e-05 training_time: 697.9\n",
      "saving info to file: ../final/MLPRegressor_600_100_40_12_-7_5.json {\n",
      "    \"cpu_info\": {\n",
      "        \"arch\": \"X86_64\",\n",
      "        \"bits\": 64,\n",
      "        \"brand_raw\": \"Intel(R) Core(TM) i7-4980HQ CPU @ 2.80GHz\",\n",
      "        \"count\": 8,\n",
      "        \"l2_cache_size\": 262144\n",
      "    },\n",
      "    \"model_params\": {\n",
      "        \"alpha\": 0.00012501661279779063,\n",
      "        \"batch_size\": 2000,\n",
      "        \"epsilon\": 3.125e-10,\n",
      "        \"hidden_layer_sizes\": [\n",
      "            600,\n",
      "            100,\n",
      "            40\n",
      "        ],\n",
      "        \"learning_rate\": \"constant\",\n",
      "        \"learning_rate_init\": 5.249607598642573e-06,\n",
      "        \"max_iter\": 400,\n",
      "        \"n_iter_no_change\": 5,\n",
      "        \"random_state\": 42,\n",
      "        \"tol\": 3.125e-07,\n",
      "        \"verbose\": true,\n",
      "        \"warm_start\": true\n",
      "    },\n",
      "    \"pca_components\": 12,\n",
      "    \"pca_unexplained_variance_ratio\": 3.734028192096107e-06,\n",
      "    \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
      "    \"test_error_absolute\": 8.941211139830143e-09,\n",
      "    \"test_error_orders_of_magnitude\": 4.7867532211924434e-05,\n",
      "    \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
      "    \"train_error_absolute\": 1.4065227327133342e-08,\n",
      "    \"train_error_orders_of_magnitude\": 4.3621245958808247e-05,\n",
      "    \"tumour_lower_size_limit_l\": 1e-07,\n",
      "    \"tumour_lower_size_limit_log10_l\": -7\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from threadpoolctl import threadpool_limits\n",
    "from cpuinfo import get_cpu_info\n",
    "import json\n",
    "\n",
    "\n",
    "hidden_layer_sizes = [600, 100, 40]\n",
    "training_start = time.time()\n",
    "for k in range(6):\n",
    "    last_file = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.pickle\"\n",
    "    info_filename = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.json\"\n",
    "    \n",
    "    if Path(last_file).exists():\n",
    "        print(f\"loading previous {last_file}\")\n",
    "        with Path(last_file).open(\"rb\") as opened_file:\n",
    "            model = pickle.load(opened_file)\n",
    "        with Path(info_filename).open('r') as opened_file:\n",
    "            print(opened_file.read())\n",
    "        continue\n",
    "    \n",
    "    if k > 0:\n",
    "        old_model = model\n",
    "    model_params = {\n",
    "        \"alpha\": 0.0040005316095293 / (2 ** k),\n",
    "        \"batch_size\": 2000,\n",
    "        \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "        \"learning_rate\": \"constant\",\n",
    "        \"learning_rate_init\": 0.00016798744315656234 / (2 ** k),\n",
    "        \"max_iter\": 400,\n",
    "        \"n_iter_no_change\": 5,\n",
    "        \"random_state\": 42,\n",
    "        \"tol\": 1e-05 / (2**k),\n",
    "        \"epsilon\": 1e-08 / (2**k),\n",
    "        \"verbose\": True,\n",
    "        \"warm_start\": k > 0\n",
    "    }\n",
    "    model = MLPRegressor(**model_params)\n",
    "    if k > 0:\n",
    "        for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "            setattr(model, variable_name, getattr(old_model, variable_name))\n",
    "        \n",
    "    with threadpool_limits(limits=get_cpu_info()[\"count\"], user_api='blas'):\n",
    "        model.fit(X_train_scaled, Y_train_pca)\n",
    "        error_test  = mean_squared_error(Y_test,  apply_size_limit(pca.inverse_transform(model.predict(X_test_scaled))))\n",
    "        error_train = mean_squared_error(Y_train, apply_size_limit(pca.inverse_transform(model.predict(X_train_scaled))))\n",
    "        error_test_absolute  = mean_squared_error(Y_test_absolute,  output_untransform(pca.inverse_transform(model.predict(X_test_scaled))))\n",
    "        error_train_absolute = mean_squared_error(Y_train_absolute, output_untransform(pca.inverse_transform(model.predict(X_train_scaled))))\n",
    "\n",
    "    print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start:.1f}\")\n",
    "        \n",
    "    with Path(last_file).open(\"wb\") as opened_file:\n",
    "        print(f\"saving {last_file}\")\n",
    "        pickle.dump(model, opened_file)\n",
    "    with Path(info_filename).open('w') as opened_file:\n",
    "        info = json.dumps({\n",
    "            \"cpu_info\": {key: get_cpu_info()[key] for key in [\"arch\", \"bits\", \"brand_raw\", \"count\", \"l2_cache_size\"]},\n",
    "            \"pca_components\": PCA_COMPONENTS,\n",
    "            \"pca_unexplained_variance_ratio\": reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0),\n",
    "            \"tumour_lower_size_limit_l\": 10 ** LOWER_LIMIT,\n",
    "            \"tumour_lower_size_limit_log10_l\": LOWER_LIMIT,\n",
    "            \"model_params\": model_params,\n",
    "            \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
    "            \"test_error_orders_of_magnitude\": error_test,\n",
    "            \"test_error_absolute\": error_test_absolute,\n",
    "            \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
    "            \"train_error_orders_of_magnitude\": error_train,\n",
    "            \"train_error_absolute\": error_train_absolute\n",
    "        }, sort_keys=True, indent=4)\n",
    "        print(f\"saving info to file: {info_filename} {info}\")\n",
    "        opened_file.write(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cd763",
   "metadata": {},
   "source": [
    "## Error comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae4e123c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th colspan='7'>test_error_orders_of_magnitude</th></tr><tr><th>LIMIT</th><th>PCA</th><th>iteration0</th><th>iteration1</th><th>iteration2</th><th>iteration3</th><th>iteration4</th></tr><tr><td>1e-07</td><td>12</td><td>6.564965133072742e-05</td><td>6.182032128073571e-05</td><td>6.39486364816562e-05</td><td>5.585245307461e-05</td><td>5.08831237005242e-05</td></tr><tr><td>1e-08</td><td>12</td><td>7.95509833552666e-05</td><td>4.976070291466626e-05</td><td>4.756687187137365e-05</td><td>4.560937754967478e-05</td><td>4.4008002234077516e-05</td></tr><tr><td>1e-09</td><td>12</td><td>5.6158462752489024e-05</td><td>5.7259925022725195e-05</td><td>5.778383797412412e-05</td><td>5.1817463314287125e-05</td><td>5.096887804159093e-05</td></tr><tr><th colspan='7'>train_error_orders_of_magnitude</th></tr><tr><th>LIMIT</th><th>PCA</th><th>iteration0</th><th>iteration1</th><th>iteration2</th><th>iteration3</th><th>iteration4</th></tr><tr><td>1e-07</td><td>12</td><td>6.08235487423917e-05</td><td>5.730799085325084e-05</td><td>5.94893830214083e-05</td><td>5.147691461318252e-05</td><td>4.662906261783197e-05</td></tr><tr><td>1e-08</td><td>12</td><td>7.578464587173801e-05</td><td>4.624160875930905e-05</td><td>4.4000776626655054e-05</td><td>4.2106657393594544e-05</td><td>4.0532013736657303e-05</td></tr><tr><td>1e-09</td><td>12</td><td>5.167492621982182e-05</td><td>5.2692317652654685e-05</td><td>5.3280704888659864e-05</td><td>4.7420734210613026e-05</td><td>4.66700791443873e-05</td></tr><tr><th colspan='7'>test_error_absolute</th></tr><tr><th>LIMIT</th><th>PCA</th><th>iteration0</th><th>iteration1</th><th>iteration2</th><th>iteration3</th><th>iteration4</th></tr><tr><td>1e-07</td><td>12</td><td>1.2594951489512178e-08</td><td>1.128093858236981e-08</td><td>1.1509219052200287e-08</td><td>1.062069063933004e-08</td><td>9.493366365736659e-09</td></tr><tr><td>1e-08</td><td>12</td><td>1.5313026484748556e-08</td><td>1.0781128156107221e-08</td><td>1.0799132976468798e-08</td><td>1.0295545912123433e-08</td><td>1.0093222568928957e-08</td></tr><tr><td>1e-09</td><td>12</td><td>1.553999355170883e-08</td><td>1.433554246371471e-08</td><td>1.4141147334981617e-08</td><td>1.3839850243592668e-08</td><td>1.3724014812159053e-08</td></tr><tr><th colspan='7'>train_error_absolute</th></tr><tr><th>LIMIT</th><th>PCA</th><th>iteration0</th><th>iteration1</th><th>iteration2</th><th>iteration3</th><th>iteration4</th></tr><tr><td>1e-07</td><td>12</td><td>2.084464684156289e-08</td><td>1.9845098849906343e-08</td><td>1.9796413951302564e-08</td><td>1.7710336381759846e-08</td><td>1.5351277365833072e-08</td></tr><tr><td>1e-08</td><td>12</td><td>2.5590254242361035e-08</td><td>2.0542437229179557e-08</td><td>2.02579368766213e-08</td><td>1.9144630181436907e-08</td><td>1.835573887695596e-08</td></tr><tr><td>1e-09</td><td>12</td><td>2.6111250474356296e-08</td><td>2.5183099569593443e-08</td><td>2.478518913056308e-08</td><td>2.4039597632026395e-08</td><td>2.389052376275253e-08</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "iterations = 5\n",
    "html = f\"<table>\"\n",
    "for label in [\"test_error_orders_of_magnitude\", \"train_error_orders_of_magnitude\", \"test_error_absolute\", \"train_error_absolute\"]:\n",
    "    html += f\"<tr><th colspan='{iterations+2}'>{label}</th></tr><tr><th>LIMIT</th><th>PCA</th>{''.join((f'<th>iteration{i}</th>') for i in range(iterations))}</tr>\"\n",
    "    for LOWER_LIMIT in [-7, -8, -9]:\n",
    "        html += f\"<tr><td>1e-0{-LOWER_LIMIT}</td><td>{PCA_COMPONENTS}</td>\"\n",
    "        for k in range(iterations):\n",
    "            info_filename = f\"../final/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.json\"\n",
    "            f = open(info_filename)\n",
    "            j = json.load(f)\n",
    "            f.close()\n",
    "            html += f\"<td>{j[label]}</td>\"\n",
    "        html += \"</tr>\"\n",
    "html += \"</table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d75248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
