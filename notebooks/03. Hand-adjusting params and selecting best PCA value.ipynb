{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we are comparing how different PCA dimensions values are affecting the performance of model `MLPRegressor`. This model is a neural network model selected in notebook `02. Training ML models`.\n",
    "\n",
    "At the same time we have given more time to the model, hand adjusted subset of the params and applied an invascaling strategy of decresing the learing rate in time to fit the model more precisely."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d879635",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_LIMIT = -7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9002c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset\n"
     ]
    }
   ],
   "source": [
    "#loading dataset\n",
    "import numpy as np\n",
    "\n",
    "input_and_output = np.load(\"../final/dataset.npz\")\n",
    "inputs  = input_and_output[\"inputs\"].astype(np.float64)\n",
    "outputs = input_and_output[\"outputs\"].astype(np.float64)\n",
    "dataset_size = inputs.shape[0]\n",
    "\n",
    "print(\"loaded dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dropping treatment column in input\n",
    "def drop_treatment(input_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Drops treatment data from the dataset\"\"\"\n",
    "    if input_data.shape[1] == 11:\n",
    "        return input_data[:, 1:]\n",
    "\n",
    "    return input_data\n",
    "\n",
    "input_without_treatment = drop_treatment(inputs)\n",
    "print(\"dropped treatment column\")\n",
    "\n",
    "\n",
    "def output_transform(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    zeros_in_output = x <= 0\n",
    "    x[zeros_in_output] = 1\n",
    "    y = np.log10(x)\n",
    "    y[zeros_in_output] = LOWER_LIMIT\n",
    "    y[y < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return y\n",
    "    \n",
    "def output_untransform(transformed_outputs: np.array) -> np.array:\n",
    "    lower_limits = transformed_outputs < LOWER_LIMIT\n",
    "    z = 10 ** transformed_outputs\n",
    "    z[lower_limits] = 0\n",
    "    return z\n",
    "\n",
    "def apply_size_limit(outputs: np.array) -> np.array:\n",
    "    x = np.copy(outputs)\n",
    "    x[x < LOWER_LIMIT] = LOWER_LIMIT\n",
    "    return x\n",
    "\n",
    "def apply_absolute_size_limit(outputs: np.array) -> np.array:\n",
    "    limit = 10 ** LOWER_LIMIT\n",
    "    x = np.copy(outputs)\n",
    "    x[x < limit] = 0\n",
    "    return x\n",
    "\n",
    "outputs_order_of_magnitude = output_transform(outputs)\n",
    "print(\"transformed to orders of magnitude\")\n",
    "\n",
    "#splitting data into train, test, validate datasets \n",
    "train_size = int(dataset_size * 0.7)\n",
    "test_size = int(dataset_size * 0.15)\n",
    "\n",
    "X_train = input_without_treatment[:train_size, :]\n",
    "Y_train_absolute = apply_absolute_size_limit(outputs[:train_size, :])\n",
    "X_test = input_without_treatment[train_size:(train_size + test_size), :]\n",
    "Y_test_absolute = apply_absolute_size_limit(outputs[train_size:(train_size + test_size), :])\n",
    "Y_train = outputs_order_of_magnitude[:train_size, :]\n",
    "Y_test = outputs_order_of_magnitude[train_size:(train_size + test_size), :]\n",
    "\n",
    "# scaling inputs\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "LOGNORMAL_PARAMETERS = (1, 2)\n",
    "\n",
    "class CustomScaler:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.plot_loval = [0.0] * len(LOGNORMAL_PARAMETERS)\n",
    "        self.plot_hival = [1.0] * len(LOGNORMAL_PARAMETERS)\n",
    "\n",
    "    def transform(self, x: np.ndarray, copy=None) -> np.ndarray:\n",
    "        res = self.scaler.transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = (x[:, parameter_index] - self.plot_loval[i]) / (self.plot_hival[i] - self.plot_loval[i])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, x, copy=None):\n",
    "        self.scaler.fit(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            column_values = x[:, parameter_index]\n",
    "\n",
    "            quantile_1, quantile_3 = np.quantile(column_values, [0.25, 0.75], axis=0)\n",
    "            iqr = quantile_3 - quantile_1\n",
    "\n",
    "            loval = quantile_1 - 1.5 * iqr\n",
    "            hival = quantile_3 + 1.5 * iqr\n",
    "\n",
    "            wiskhi = np.compress(column_values <= hival, column_values)\n",
    "            wisklo = np.compress(column_values >= loval, column_values)\n",
    "            actual_hival = np.max(wiskhi)\n",
    "            actual_loval = np.min(wisklo)\n",
    "\n",
    "            self.plot_loval[i] = actual_loval\n",
    "            self.plot_hival[i] = actual_hival\n",
    "\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, x, copy=None):\n",
    "        res = self.scaler.inverse_transform(x)\n",
    "        for i, parameter_index in enumerate(LOGNORMAL_PARAMETERS):\n",
    "            res[:, parameter_index] = x[:, parameter_index] * (self.plot_hival[i] - self.plot_loval[i]) + self.plot_loval[i]\n",
    "        return res\n",
    "\n",
    "scaler_path = Path(f\"../final/scaler.pickle\")\n",
    "scaler = None\n",
    "if scaler_path.exists():\n",
    "    with scaler_path.open(\"rb\") as scaler_file:\n",
    "        scaler = pickle.load(scaler_file)\n",
    "else:\n",
    "    scaler = CustomScaler().fit(X_train)\n",
    "    with scaler_path.open(\"wb\") as opened_file:\n",
    "        pickle.dump(scaler, opened_file)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"scaled\")\n",
    "\n",
    "iterations = 7\n",
    "\n",
    "Path(\"../final/pca_models\").mkdir(parents=True, exist_ok=True)\n",
    "for PCA_COMPONENTS in [8, 10, 11, 12, 13, 16, 20]:\n",
    "    # applying principal component analysis\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca_path = Path(f\"../final/pca/{PCA_COMPONENTS}_{LOWER_LIMIT}.pickle\")\n",
    "\n",
    "    if pca_path.exists():\n",
    "        with pca_path.open(\"rb\") as opened_file:\n",
    "            pca = pickle.load(opened_file)\n",
    "        Y_train_pca = pca.transform(Y_train)\n",
    "    else: \n",
    "        pca = PCA(n_components=PCA_COMPONENTS)\n",
    "        Y_train_pca = pca.fit_transform(Y_train)\n",
    "        with pca_path.open(\"wb\") as opened_file:\n",
    "            pickle.dump(pca, opened_file)\n",
    "\n",
    "    from functools import reduce\n",
    "    print(f\"applied pca with {PCA_COMPONENTS} components. Unexplained variance ratio: {reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0)}\")\n",
    "\n",
    "    import time\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from threadpoolctl import threadpool_limits\n",
    "    from cpuinfo import get_cpu_info\n",
    "    import json\n",
    "\n",
    "    hidden_layer_sizes = [600, 100, 40]\n",
    "    training_start = time.time()\n",
    "    for k in range(iterations):\n",
    "        last_file = f\"../final/pca_models/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.pickle\"\n",
    "        info_filename = f\"../final/pca_models/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.json\"\n",
    "\n",
    "        if Path(last_file).exists():\n",
    "            print(f\"loading previous {last_file}\")\n",
    "            with Path(last_file).open(\"rb\") as opened_file:\n",
    "                model = pickle.load(opened_file)\n",
    "            with Path(info_filename).open('r') as opened_file:\n",
    "                print(opened_file.read())\n",
    "            continue\n",
    "\n",
    "        if k > 0:\n",
    "            old_model = model\n",
    "        model_params = {\n",
    "            \"alpha\": 0.0040005316095293 / (2 ** k),\n",
    "            \"batch_size\": 2000,\n",
    "            \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "            \"learning_rate\": \"constant\",\n",
    "            \"learning_rate_init\": 0.00016798744315656234 / (2 ** k),\n",
    "            \"max_iter\": 400,\n",
    "            \"n_iter_no_change\": 5,\n",
    "            \"random_state\": 42,\n",
    "            \"tol\": 1e-05 / (2**k),\n",
    "            \"epsilon\": 1e-08 / (2**k),\n",
    "            \"verbose\": True,\n",
    "            \"warm_start\": k > 0\n",
    "        }\n",
    "        model = MLPRegressor(**model_params)\n",
    "        if k > 0:\n",
    "            for variable_name in (\"coefs_\", \"t_\", \"n_outputs_\", \"n_layers_\", \"out_activation_\", \"intercepts_\", \"n_iter_\", \"loss_curve_\", \"best_loss_\", \"_no_improvement_count\"):\n",
    "                setattr(model, variable_name, getattr(old_model, variable_name))\n",
    "\n",
    "        with threadpool_limits(limits=get_cpu_info()[\"count\"], user_api='blas'):\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train_scaled, Y_train_pca)\n",
    "            training_time_s = int(time.time() - start_time)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            test_result = pca.inverse_transform(model.predict(X_test_scaled))\n",
    "            error_test  = mean_squared_error(Y_test,  apply_size_limit(test_result))\n",
    "            error_test_absolute  = mean_squared_error(Y_test_absolute,  output_untransform(test_result))\n",
    "            test_evaluation_s = int(time.time() - start_time)\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_result = pca.inverse_transform(model.predict(X_train_scaled))\n",
    "            error_train = mean_squared_error(Y_train, apply_size_limit(train_result))\n",
    "            error_train_absolute = mean_squared_error(Y_train_absolute, output_untransform(train_result))\n",
    "            train_evaluation_s = int(time.time() - start_time)\n",
    "\n",
    "        print(f\"error test: {error_test}, error train: {error_train} training_time: {time.time() - training_start:.1f}\")\n",
    "\n",
    "        with Path(last_file).open(\"wb\") as opened_file:\n",
    "            print(f\"saving {last_file}\")\n",
    "            pickle.dump(model, opened_file)\n",
    "        with Path(info_filename).open('w') as opened_file:\n",
    "            info = json.dumps({\n",
    "                \"cpu_info\": {key: get_cpu_info()[key] for key in [\"arch\", \"bits\", \"brand_raw\", \"count\", \"l2_cache_size\"]},\n",
    "                \"pca_components\": PCA_COMPONENTS,\n",
    "                \"pca_unexplained_variance_ratio\": reduce(lambda a, b: a - b, pca.explained_variance_ratio_, 1.0),\n",
    "                \"tumour_lower_size_limit_l\": 10 ** LOWER_LIMIT,\n",
    "                \"tumour_lower_size_limit_log10_l\": LOWER_LIMIT,\n",
    "                \"model_params\": model_params,\n",
    "                \"test_dataset\": \"[700000:850000] of ../final/dataset.npz\",\n",
    "                \"test_error_orders_of_magnitude\": error_test,\n",
    "                \"test_error_absolute\": error_test_absolute,\n",
    "                \"train_dataset\": \"[:700000] of ../final/dataset.npz\",\n",
    "                \"train_error_orders_of_magnitude\": error_train,\n",
    "                \"train_error_absolute\": error_train_absolute,\n",
    "                \"training_time_s\": training_time_s,\n",
    "                \"train_evaluation_s\": train_evaluation_s,\n",
    "                \"test_evaluation_s\": test_evaluation_s\n",
    "            }, sort_keys=True, indent=4)\n",
    "            print(f\"saving info to file: {info_filename} {info}\")\n",
    "            opened_file.write(info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfeab329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th colspan='8'>test_error_orders_of_magnitude</th></tr><tr><th>PCA</th><th>iteration0</th><th>iteration1</th><th>iteration2</th><th>iteration3</th><th>iteration4</th><th>iteration5</th><th>iteration6</th></tr><tr><td>8</td><td>6.066106451667655e-05</td><td>6.066106451667655e-05</td><td>5.731222135071783e-05</td><td>4.698716297157301e-05</td><td>4.255607590444751e-05</td><td>4.0443365571298086e-05</td><td>4.0194493961427285e-05</td></tr><tr><td>10</td><td>6.899643959716373e-05</td><td>6.791009645367844e-05</td><td>5.8540865961222866e-05</td><td>5.808633976250023e-05</td><td>4.708323503686907e-05</td><td>4.56252354467719e-05</td><td>4.285595117220148e-05</td></tr><tr><td>11</td><td>6.794408038885003e-05</td><td>6.790194572731308e-05</td><td>6.630579304226004e-05</td><td>4.7737728616053614e-05</td><td>4.676234574313509e-05</td><td>3.934072051074781e-05</td><td>3.931033215263559e-05</td></tr><tr><td>12</td><td>6.557393853416707e-05</td><td>6.180271055505593e-05</td><td>6.395702193943817e-05</td><td>5.593207897286973e-05</td><td>5.115655268962934e-05</td><td>4.9418816157761024e-05</td><td>4.667146805886745e-05</td></tr><tr><td>13</td><td>0.00011828451477292318</td><td>6.904487771340471e-05</td><td>6.850588595475479e-05</td><td>5.820191772779284e-05</td><td>5.530160087643854e-05</td><td>4.9898225813367146e-05</td><td>4.796358595940089e-05</td></tr><tr><td>16</td><td>6.464681256249444e-05</td><td>6.147670534781553e-05</td><td>5.986221861026651e-05</td><td>5.617812195574911e-05</td><td>5.360255726385988e-05</td><td>5.1931972311369013e-05</td><td>4.6062689509678724e-05</td></tr><tr><td>20</td><td>9.655420209746684e-05</td><td>7.73948837225179e-05</td><td>7.499215899363186e-05</td><td>7.204930744406215e-05</td><td>6.807789843031294e-05</td><td>6.190619841283079e-05</td><td>5.136809610865824e-05</td></tr><tr><th colspan='8'>train_error_orders_of_magnitude</th></tr><tr><th>PCA</th><th>iteration0</th><th>iteration1</th><th>iteration2</th><th>iteration3</th><th>iteration4</th><th>iteration5</th><th>iteration6</th></tr><tr><td>8</td><td>5.779183179228414e-05</td><td>5.779183179228414e-05</td><td>5.4281500800019304e-05</td><td>4.433434659605816e-05</td><td>4.000329580354935e-05</td><td>3.7960087254247375e-05</td><td>3.771113611329628e-05</td></tr><tr><td>10</td><td>6.343244583063155e-05</td><td>6.329394119730922e-05</td><td>5.406223658829814e-05</td><td>5.3663490966086855e-05</td><td>4.3165337791501914e-05</td><td>4.170593346606834e-05</td><td>3.9148711577339964e-05</td></tr><tr><td>11</td><td>6.46377637234048e-05</td><td>6.46009481272536e-05</td><td>6.31526006606918e-05</td><td>4.4993240437802834e-05</td><td>4.3996966417453926e-05</td><td>3.7008175068316976e-05</td><td>3.693785661675204e-05</td></tr><tr><td>12</td><td>6.075924183173974e-05</td><td>5.7284130962455374e-05</td><td>5.951337827692913e-05</td><td>5.1591554755312395e-05</td><td>4.684383861821968e-05</td><td>4.5226056511155825e-05</td><td>4.2500460320973596e-05</td></tr><tr><td>13</td><td>0.00011348470771225395</td><td>6.474620468108922e-05</td><td>6.46167903269252e-05</td><td>5.473287493718769e-05</td><td>5.163173708187916e-05</td><td>4.677204320865731e-05</td><td>4.493857804982329e-05</td></tr><tr><td>16</td><td>5.990566891209204e-05</td><td>5.7237746226846e-05</td><td>5.576152266897642e-05</td><td>5.193898722099576e-05</td><td>4.93231004378674e-05</td><td>4.780618495382435e-05</td><td>4.218866187591784e-05</td></tr><tr><td>20</td><td>9.282986011056635e-05</td><td>7.377483145308472e-05</td><td>7.141063487665613e-05</td><td>6.838805724184083e-05</td><td>6.432758777886785e-05</td><td>5.844304456883704e-05</td><td>4.816775369258416e-05</td></tr><tr><th colspan='8'>test_error_absolute</th></tr><tr><th>PCA</th><th>iteration0</th><th>iteration1</th><th>iteration2</th><th>iteration3</th><th>iteration4</th><th>iteration5</th><th>iteration6</th></tr><tr><td>8</td><td>9.796188781447963e-09</td><td>9.796188781447963e-09</td><td>9.366301745555178e-09</td><td>7.916800975602806e-09</td><td>7.261736759680675e-09</td><td>7.066377251014791e-09</td><td>7.015200272699757e-09</td></tr><tr><td>10</td><td>1.6973813067330186e-08</td><td>1.697511225106885e-08</td><td>1.4911087169553134e-08</td><td>1.416082540886646e-08</td><td>1.1715099080984186e-08</td><td>1.1309620880848294e-08</td><td>1.0804412263693171e-08</td></tr><tr><td>11</td><td>1.2279100751351284e-08</td><td>1.142394642050325e-08</td><td>1.1699945454498013e-08</td><td>8.909071426015161e-09</td><td>8.50544265348525e-09</td><td>7.448601510594944e-09</td><td>7.22478490194604e-09</td></tr><tr><td>12</td><td>1.253259420015431e-08</td><td>1.1254185534870018e-08</td><td>1.151401681499883e-08</td><td>1.0734760676301487e-08</td><td>9.44106495766028e-09</td><td>9.140147917783133e-09</td><td>8.599907065003592e-09</td></tr><tr><td>13</td><td>3.110875753789576e-08</td><td>1.6932709565798262e-08</td><td>1.6587465999375963e-08</td><td>1.537240517821454e-08</td><td>1.4518615528193425e-08</td><td>1.3393174605533372e-08</td><td>1.3039170808714503e-08</td></tr><tr><td>16</td><td>1.876841980189005e-08</td><td>1.8588746633484804e-08</td><td>1.7029087466234434e-08</td><td>1.642735218569972e-08</td><td>1.5511530911862896e-08</td><td>1.4993521364487192e-08</td><td>1.2875550267127385e-08</td></tr><tr><td>20</td><td>2.7505084240217224e-08</td><td>2.411436702895643e-08</td><td>2.3903625176127316e-08</td><td>2.2455134533628353e-08</td><td>1.979775590991543e-08</td><td>1.5911577714772092e-08</td><td>1.1241178394608875e-08</td></tr><tr><th colspan='8'>train_error_absolute</th></tr><tr><th>PCA</th><th>iteration0</th><th>iteration1</th><th>iteration2</th><th>iteration3</th><th>iteration4</th><th>iteration5</th><th>iteration6</th></tr><tr><td>8</td><td>1.4916779570062616e-08</td><td>1.4916779570062616e-08</td><td>1.4258325816528787e-08</td><td>1.1210623383641083e-08</td><td>1.045121865932745e-08</td><td>9.85892118165612e-09</td><td>9.824910727342343e-09</td></tr><tr><td>10</td><td>2.5076599402781042e-08</td><td>2.458278088393571e-08</td><td>2.2053772304255793e-08</td><td>2.101489734574099e-08</td><td>1.783919446314968e-08</td><td>1.7179974116342993e-08</td><td>1.6151937829715714e-08</td></tr><tr><td>11</td><td>2.072028459440125e-08</td><td>1.9495470261707083e-08</td><td>1.952758922415324e-08</td><td>1.4044862219269963e-08</td><td>1.3646538739259681e-08</td><td>1.1525157720052371e-08</td><td>1.1454052093262647e-08</td></tr><tr><td>12</td><td>2.0738278568857586e-08</td><td>1.9715174066729802e-08</td><td>1.9734855343060974e-08</td><td>1.7775799411962334e-08</td><td>1.522186568666752e-08</td><td>1.4726962347116647e-08</td><td>1.3727111144307046e-08</td></tr><tr><td>13</td><td>3.945563087322139e-08</td><td>2.781349109827677e-08</td><td>2.7127580463511313e-08</td><td>2.471797653092663e-08</td><td>2.3663484057801052e-08</td><td>2.2022234173930297e-08</td><td>2.1385713673583243e-08</td></tr><tr><td>16</td><td>2.7069020413831203e-08</td><td>2.7251904940492462e-08</td><td>2.5756608049428404e-08</td><td>2.4867496339380148e-08</td><td>2.3489307931672064e-08</td><td>2.2795967283536147e-08</td><td>1.9960744413039292e-08</td></tr><tr><td>20</td><td>4.774064459383705e-08</td><td>4.31470776409022e-08</td><td>4.284386674100363e-08</td><td>4.041447405494014e-08</td><td>3.5567383806575765e-08</td><td>2.8179277198230237e-08</td><td>1.9109116740707396e-08</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "html = f\"<table>\"\n",
    "for label in [\"test_error_orders_of_magnitude\", \"train_error_orders_of_magnitude\", \"test_error_absolute\", \"train_error_absolute\"]:\n",
    "    html += f\"<tr><th colspan='{iterations+1}'>{label}</th></tr><tr><th>PCA</th>{''.join((f'<th>iteration{i}</th>') for i in range(iterations))}</tr>\"\n",
    "    for PCA_COMPONENTS in [8, 10, 11, 12, 13, 16, 20]:\n",
    "        html += f\"<tr><td>{PCA_COMPONENTS}</td>\"\n",
    "        for k in range(iterations):\n",
    "            info_filename = f\"../final/pca_models/MLPRegressor_{'_'.join(str(i) for i in hidden_layer_sizes)}_{PCA_COMPONENTS}_{LOWER_LIMIT}_{k}.json\"\n",
    "            f = open(info_filename)\n",
    "            j = json.load(f)\n",
    "            f.close()\n",
    "            html += f\"<td>{j[label]}</td>\"\n",
    "        html += \"</tr>\"\n",
    "html += \"</table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d51a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
